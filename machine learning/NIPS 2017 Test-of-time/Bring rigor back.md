06年左右，dl和svm等研究处于一个不能scale up的阶段

甚至很难可以reproducible的baseline

"nips rigor police"

"Al is the new alchemy"

two-layer neural network, linear layers
Levenberg-Marquardt (1944) versus SGD (1951) versus ADAM (2014)

batch norm, "reducing internal covariate shift"

听 project 课的答辩，绝大部分组停留在我们尝试了什么方法，一些工作，另一些不工作的尝试，尝试去解释为什么不被考虑进去。

理论落后于实验，这并不意味着可以把理论抛在一边。

深度学习的范式不一样：不是化整为零的思路，不用控制变量，而是多变量变化（网络结构）

类似的关系：中药和西医

具体讨论不稳定性数据的论文： https://zhuanlan.zhihu.com/p/31546065

